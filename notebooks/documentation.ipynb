{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be951cf-496a-4584-a965-d1d3c118abed",
   "metadata": {},
   "source": [
    "# **VisTool Documentation**\n",
    "\n",
    "Welcome to the **VisTool** user guide! This documentation provides an overview of the package, guidance and a tutorial on how to use the key functionalities.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**  \n",
    "\n",
    "**VisTool** is a Python package designed to provide high-level visualisation tools for analysing health datasets. This toolkit enables users to:  \n",
    "- Easily load and clean health-related datasets.  \n",
    "- Customise visualisations for better insights and decision-making.  \n",
    "- Merge, concatenate and manipulate datasets with ease.  \n",
    "- Leverage interactive tools for dynamic data exploration.  \n",
    "\n",
    "## Why use this package?\n",
    "\n",
    "In today's data-driven world, the ability to transform raw health data into actionable insights is critical. **VisTool** addresses this need by extracting meaningful insights from your data and offering:\n",
    "- Simplicity\n",
    "- Efficiency\n",
    "- Customisation\n",
    "- Relevance towards Healthcare Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c093d0-9257-4379-b054-b4eec5a6bb22",
   "metadata": {},
   "source": [
    "# **Functionality Overview**\n",
    "\n",
    "## **Modules and Their Functions**\n",
    "\n",
    "### **1. Download Module**\n",
    "This module provides utilities for downloading files from the internet and working with them locally. It simplifies the process of retrieving datasets, making it easier to integrate data from external sources.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `download_file(url, save_path)`: Downloads a file from a specified URL and saves it to a provided path.\n",
    "- `download_csv(url)`: Downloads a CSV file from a URL and loads it directly into a Pandas DataFrame.\n",
    "\n",
    "#### **Usage Instructions**\n",
    "1. Navigate to the `example_usage.ipynb` notebook.\n",
    "2. Locate the \"Imports to be ran\" block and execute the code.\n",
    "3. Scroll down to the **Download Module** section.\n",
    "4. Find a file to download and copy its URL.\n",
    "5. Provide a save path for the file, e.g.,:\n",
    "   ```python\n",
    "   url = \"https://people.sc.fsu.edu/~jburkardt/data/csv/airtravel.csv\"\n",
    "   save_path = \"data/airtravel.csv\"\n",
    "   download_file(url, save_path)\n",
    "   ```\n",
    "6. To download and load a CSV into a DataFrame:\n",
    "   ```python\n",
    "   url = \"https://people.sc.fsu.edu/~jburkardt/data/csv/airtravel.csv\"\n",
    "   df = download_csv(url)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Combine Module**\n",
    "This module provides functions to merge or concatenate datasets, enabling seamless integration of multiple data sources.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `merge_datasets(data1, data2, on, how)`: Combines datasets based on a common column or index with a specified join type (e.g., left, inner).\n",
    "- `concat_datasets(datasets, axis)`: Concatenates datasets along rows or columns.\n",
    "\n",
    "#### **Usage Instructions**\n",
    "1. Navigate to the `example_usage.ipynb` notebook.\n",
    "2. Scroll to the **Combine Module** section.\n",
    "3. Ensure you have the datasets to combine.\n",
    "4. Merge datasets:\n",
    "   ```python\n",
    "   merged_data = merge_datasets(data1, data2, on=\"primary_id\", how=\"inner\")\n",
    "   ```\n",
    "5. Concatenate datasets:\n",
    "   ```python\n",
    "   concatenated_data = concat_datasets([data3, data4], axis=1)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Wrangle Module**\n",
    "This module streamlines pre-processing tasks such as cleaning and transforming datasets. It enables users to focus on insights rather than data inconsistencies.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `clean_data(data, apply_to)`: Cleans and standardises datasets.\n",
    "- `filter_data(data, condition)`: Filters data based on specific conditions.\n",
    "- `rename_columns(data, columns_mapping)`: Renames columns for clarity.\n",
    "- `label_encode(data, column)`: Perform label encoding on a categorical column using Pandas and NumPy.\n",
    "\n",
    "#### **Usage Instructions**\n",
    "1. Navigate to the `example_usage.ipynb` notebook.\n",
    "2. Scroll to the **Wrangle Module** section and run the code block.\n",
    "3. Select a data wrangling option (e.g., removing or filling NaN values).\n",
    "4. Follow prompts to preview and clean the dataset:\n",
    "   - Example: Remove rows with NaN values.\n",
    "   ```python\n",
    "   cleaned_data = clean_data(remove_nan=True, axis=0)\n",
    "   ```\n",
    "5. Filter data with a condition:\n",
    "   ```python\n",
    "   filtered_data = filter_data(\"other_emergency_admissions > 100\")\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visualize Module**\n",
    "This module simplifies the creation of visualisations for exploratory data analysis.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `plot_histogram(data, column)`: Visualises the distribution of values in a column.\n",
    "- `plot_scatter(data, x_column, y_column)`: Examines the relationship between two numeric variables.\n",
    "- `plot_correlation_matrix(data)`: Analyses correlations among numeric columns.\n",
    "- `plot_line(data, x_column, y_column)`: Visualises trends or changes over time.\n",
    "- `plot_overlay(data, columns, plot_types)`: Overlays multiple plots for comparison.\n",
    "\n",
    "#### **Usage Instructions**\n",
    "1. Navigate to the **Visualize Module** section in `example_usage.ipynb`.\n",
    "2. Choose a graph type (e.g., histogram, scatter, correlation matrix).\n",
    "3. User will then see available columns to choose from within the NHS A&E Attendances dataset.\n",
    "4. Specify the columns for plotting:\n",
    "   ```python\n",
    "   plot_histogram(\"admission_counts\")\n",
    "   plot_scatter(\"time\", \"admission_counts\")\n",
    "   ```\n",
    "5. Overlay plots for comparison:\n",
    "   ```python\n",
    "   plot_overlay([\"column1\", \"column2\"], [\"line\", \"bar\"])\n",
    "   ```\n",
    "6. User will be prompted whether they would like to save the plot in the existing folder location.\n",
    "\n",
    "---\n",
    "\n",
    "***If you wish for some further in-depth usage examples, please make your way to the `advanced_example_usage.ipynb` notebook.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df34cd-6a3f-4889-a4fb-7cebc824fd74",
   "metadata": {},
   "source": [
    "# Testing Documentation\n",
    "\n",
    "This section outlines the testing strategies employed to ensure the robustness and reliability of the project. Various types of tests were conducted, including unit tests and functional tests. The goal of these were to verify that individual components function as expected.\n",
    "\n",
    "## Unit Testing - pytest\n",
    "Unit tests using pytest were implemented for the key modules of the project to verify the correctness of individual functions. Below is a summary of the unit tests conducted:\n",
    "\n",
    "### 1. `test_combine.py`\n",
    "This module tests the dataset merging and concatenation functionality provided by `merge_datasets` and `concat_datasets`.\n",
    "\n",
    "#### Key Tests:\n",
    "- **`test_merge_datasets`**:\n",
    "  - Validates the merging of two datasets based on a specified column and join type.\n",
    "  - Ensures the end dataset contains the correct number of rows and columns.\n",
    "  - Confirms that the expected columns exist in the merged dataset.\n",
    "- **`test_concat_datasets`**:\n",
    "  - Tests the concatenation of two datasets along a specified axis.\n",
    "  - Verifies the dimensions and column names of the concatenated dataset.\n",
    "\n",
    "### 2. `test_download.py`\n",
    "This module tests the data downloading functionalities provided by `download_file` and `download_csv`.\n",
    "\n",
    "#### Key Tests:\n",
    "- **`test_download_file`**:\n",
    "  - Ensures a file can be downloaded from a valid URL and saved locally.\n",
    "  - Confirms that the file exists and is not empty after download.\n",
    "- **`test_download_csv`**:\n",
    "  - Tests downloading a CSV file and loading it into a pandas DataFrame.\n",
    "  - Checks the existence of expected columns and ensures the DataFrame is not empty.\n",
    "- **`test_download_invalid_url`**:\n",
    "  - Verifies that an exception is raised when an invalid URL is provided.\n",
    "\n",
    "### 3. `test_visualize.py`\n",
    "This module tests the visualisation functionalities, including plotting histograms, scatter plots, correlation matrices, line charts and overlay plots.\n",
    "\n",
    "#### Key Tests:\n",
    "- **`test_plot_histogram`**:\n",
    "  - Validates histogram generation with and without saving the plot.\n",
    "- **`test_plot_scatter`**:\n",
    "  - Ensures scatter plot generation and saving functionality.\n",
    "- **`test_plot_correlation_matrix`**:\n",
    "  - Verifies the creation of a correlation matrix plot and saving functionality.\n",
    "- **`test_plot_line`**:\n",
    "  - Tests the generation of line charts with and without saving the plot.\n",
    "- **`test_plot_overlay`**:\n",
    "  - Ensures the creation of overlay plots with specified types and saving functionality.\n",
    "\n",
    "### 4. `test_wrangle.py`\n",
    "This module tests the data wrangling functionalities, including data cleaning, filtering and column renaming.\n",
    "\n",
    "#### Key Tests:\n",
    "- **`test_clean_data`**:\n",
    "  - Ensures data is cleaned by removing rows with NaN values or filling NaN values with specified methods (e.g., mean).\n",
    "  - Validates that an exception is raised for invalid cleaning options.\n",
    "- **`test_filter_data`**:\n",
    "  - Tests filtering of data based on a condition and validates edge cases such as invalid conditions.\n",
    "- **`test_rename_columns`**:\n",
    "  - Confirms that columns are renamed correctly and validates behaviour for invalid column mappings.\n",
    "\n",
    "\n",
    "#### How to run pytest:\n",
    "\n",
    "To ensure the package is running as expected, you can can run the provided test suite using pytest. \n",
    "\n",
    "1. Open a terminal and navigate to the root directory of the project where `tests/` folder is located.\n",
    "2. Run the following command if you wish to run them all:\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "3. If you wish to run all tests but with a detailed output:\n",
    "```bash\n",
    "pytest -v\n",
    "```\n",
    "4. If you wish to isolate single tests:\n",
    "```bash\n",
    "pytest tests/test_visualize.py\n",
    "```\n",
    "\n",
    "## Functional Testing\n",
    "Functional tests were conducted using Excel to simulate real-world scenarios and ensure that the functions behave as expected with various inputs.\n",
    "\n",
    "### Functional Testing Process:\n",
    "1. Inputs were manually supplied in Excel to test the functions.\n",
    "2. Outputs were generated and compared against expected results.\n",
    "3. Any discrepancies between actual and expected results were documented.\n",
    "\n",
    "### Example Functional Test Cases:\n",
    "- **Merge Datasets:**\n",
    "  - Inputs: Two datasets with overlapping `id` columns.\n",
    "  - Expected Output: Merged dataset with only matching rows.\n",
    "- **Download CSV:**\n",
    "  - Input: A valid CSV file URL.\n",
    "  - Expected Output: A non-empty DataFrame with specific column names.\n",
    "- **Plot Histogram:**\n",
    "  - Input: A dataset column with numerical values.\n",
    "  - Expected Output: A histogram plot with accurate representation of value distribution.\n",
    "\n",
    "## Summary\n",
    "The combination of unit and functional testing ensures that all components of the project function as intended under various conditions. The rigorous stress testing process also facilitates early detection of issues, improving the overall reliability of the package.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c848f-45c2-43bb-8895-3cedeb593f61",
   "metadata": {},
   "source": [
    "Whether you're a data analyst, healthcare researcher, or BI professional, VisTool empowers you to extract meaningful insights from your data. By bridging the gap between data preparation and analysis, this package helps users make informed decisions that can drive impactful outcomes in the healthcare domain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb505ab-8fc6-4f12-9e7c-f0e99bea25d6",
   "metadata": {},
   "source": [
    "# Support\n",
    "\n",
    "If you encounter any issues, please raise them in the GitHub Issues section.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034432cc-9e7e-4c87-9ac9-eaffa0101ef9",
   "metadata": {},
   "source": [
    "# Contributing\n",
    "\n",
    "We welcome contributions to VisTool. Please fork the repository, make your changes and submit a pull request.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09958b-d40c-4be3-b20d-ec885a2fd6bc",
   "metadata": {},
   "source": [
    "<i> Written By: Kayleigh Haydock</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
